# ChatGpt â€” Real-time AI Chat Backend âœ¨ğŸ¤–

Summary ğŸ§­
- Purpose: Node.js backend for an AI chat app with user auth, chat threads, real-time replies via Socket.IO, and long-term memory using Pinecone. âœï¸
- Status: MVP in progress. ğŸš¦
- Owner(s): Yashaswirai. ğŸ‘¤
- Live URLs: Not deployed yet. ğŸ”—
- Repo: Monorepo not used; this is a single service. ğŸ—‚ï¸

## Table of Contents ğŸ“š
- Overview ğŸš€
- Features âœ¨
- Architecture ğŸ—ï¸
- Project Layout ğŸ—‚ï¸
- Setup âš™ï¸
- Configuration ğŸ”§
- Usage â–¶ï¸
- API ğŸ”Œ
- Sockets ğŸ”Œâš¡
- Data Model ğŸ—ƒï¸
- Prompting & AI Integration ğŸ¤–ğŸ§ 
- Quality (Testing, Linting) âœ…
- Build & Deploy ğŸš¢
- Security & Compliance ğŸ”
- Observability ğŸ‘€
- Performance & Cost âš¡
- Roadmap ğŸ—ºï¸
- Changelog ğŸ“
- License ğŸ“„
- FAQ & Troubleshooting â“ğŸ’¡

## Overview ğŸš€
- Problem: Provide a backend that can authenticate users, organize chats, generate AI responses, and retain context across sessions. ğŸ¯
- Solution: Express + MongoDB API, Socket.IO for real-time chat, Google GenAI for text generation and embeddings, Pinecone for vector memory (RAG-style retrieval). ğŸ§©
- Non-goals: Frontend/UI, multi-provider abstraction, role-based admin panel. ğŸš«

## Features âœ¨
Current
- JWT auth with cookies (register/login). ğŸŒŸ
- Chat thread creation and message persistence. ğŸŒŸ
- Real-time AI replies over WebSocket events. ğŸŒŸ
- Long-term memory via Pinecone with embeddings from Google GenAI. ğŸŒŸ

Planned
- Streaming token responses. ğŸ—“ï¸
- Message and chat management REST endpoints. ğŸ—“ï¸
- Rate limiting and input validation. ğŸ—“ï¸

## Architecture ğŸ—ï¸
High-level
- Backend/API: Express 5 on Node.js; Socket.IO for realtime. ğŸ§±
- AI Provider: Google GenAI
  - Text model: `gemini-2.5-flash`
  - Embedding model: `gemini-embedding-001`
- Data: MongoDB (Mongoose) for users/chats/messages; Pinecone index `chatgpt` for vectors (dim 768). ğŸ—„ï¸
- Auth: JWT, stored in `token` cookie for HTTP and Socket.IO handshake. ğŸ”
- Observability: Console logging. ğŸ“Š

Data flow ğŸ”„
1) Client emits `ai-message` with `{ chatId, content }`.
2) Server saves the user message and embeds it; upserts into Pinecone. 
3) Server fetches top-k related memories and recent chat history.
4) Prompt is built from long-term memory (LTM) + short-term history (STM).
5) Google GenAI generates the response; server emits `ai-response` and persists it; embedding stored to Pinecone.

Key decisions ğŸ§ 
- Google GenAI for cost/speed; Pinecone for scalable vector memory.
- Fixed topK memory retrieval; last 7 messages for STM.

## Project Layout ğŸ—‚ï¸
```
package.json
Readme.md
server.js
src/
  app.js
  controllers/
    auth.controller.js
    chat.controller.js
  db/
    db.js
  middlewares/
    auth.middleware.js
  models/
    chat.model.js
    message.model.js
    user.model.js
  routes/
    auth.routes.js
    chat.routes.js
  services/
    Ai.service.js
    Vector.service.js
  socket/
    socket.server.js
```

## Setup âš™ï¸
Prerequisites ğŸ§°
- Node.js LTS (18+ recommended) and npm
- MongoDB database (Atlas/local)
- Pinecone account with index `chatgpt` (dimension 768)
- Google API key for Generative AI

Install dependencies ğŸ“¦
- Windows PowerShell:
  - npm install

Environment ğŸŒ±
- Create a `.env` file in repo root:
  - PORT=3000
  - MONGODB_URI="mongodb+srv://<user>:<pass>@<cluster>/<db>?retryWrites=true&w=majority"
  - JWT_SECRET="your_jwt_secret"
  - GOOGLE_API_KEY="your_google_genai_api_key"
  - PINECONE_API_KEY="your_pinecone_api_key"

Start dev â–¶ï¸
- npm run dev
- Server listens on `http://localhost:<PORT>` (default 3000)

## Configuration ğŸ”§
Environment variables ğŸ”‘
- PORT: HTTP port (default 3000)
- MONGODB_URI: MongoDB connection string
- JWT_SECRET: Secret to sign/verify JWTs
- GOOGLE_API_KEY: Google Generative AI API key (used by `@google/genai`)
- PINECONE_API_KEY: Pinecone API key (index name `chatgpt`)

Notes
- Ensure Pinecone index `chatgpt` exists with dimension 768.
- Cookie name is `token`; set `withCredentials` on frontend requests/sockets if cross-origin.

## Usage â–¶ï¸
HTTP base URL
- `/` GET â†’ Health/info: returns "Welcome to the MERN-GPT API".

Auth flow (cookies)
1) POST `/api/auth/register` â†’ sets `token` cookie
2) POST `/api/auth/login` â†’ sets `token` cookie
3) Use cookie-authenticated requests and socket connections

Create a chat
- POST `/api/chat` (auth) with `{ "title": "My Chat" }` â†’ returns new chat

## API ğŸ”Œ
Conventions
- JSON; cookie-based JWT; status codes on failures.

Endpoints ğŸ“¡
- POST `/api/auth/register`
  - Body: `{ username, email, password }`
  - 201, sets `token` cookie; returns `newUser`
- POST `/api/auth/login`
  - Body: `{ identifier, password }` (identifier = username or email)
  - 200, sets `token` cookie; returns `user`
- POST `/api/chat` (auth required)
  - Body: `{ title }`
  - 201; returns `{ chat }`

Errors ğŸ§¯
- 400 duplicate user, 401 unauthorized, 404 user not found, 500 server errors

## Sockets ğŸ”Œâš¡
Namespace: default
- Auth: via `token` cookie parsed during Socket.IO handshake.

Events
- Client â†’ Server: `ai-message`
  - Payload: `{ chatId: string, content: string }`
- Server â†’ Client: `ai-response`
  - Payload: `{ chatId: string, response: string }`

Flow
1) Server stores the user message (MongoDB) and upserts its embedding (Pinecone).
2) Retrieves top-K related memories and last 7 chat messages.
3) Builds prompt (LTM + STM), calls Google GenAI, emits `ai-response`.
4) Persists model response and its embedding.

## Data Model ğŸ—ƒï¸
User
- username (unique, required)
- email (unique, required)
- password (hash)

Chat
- userId (ref User, required)
- title (required)

Message
- chatId (ref Chat, required)
- sender (ref User, required)
- content (string, required)
- role ("user" | "model" | "system")

## Prompting & AI Integration ğŸ¤–ğŸ§ 
System/Context
- Long-term memory injected as a system-style message with concatenated previous contents from Pinecone matches.

Message formatting
- Google GenAI `contents`: each message as `{ role, parts: [{ text }] }`.
- STM: last 7 messages; LTM: memory from Pinecone.

Models
- Text: `gemini-2.5-flash`
- Embedding: `gemini-embedding-001` with outputDimensionality 768

## Quality (Testing, Linting) âœ…
- No tests or linters configured yet.
- Suggested next: add ESLint + basic unit tests for controllers and middleware.

## Build & Deploy ğŸš¢
- Node service, no build step required.
- Provide `.env` at runtime and reachable MongoDB and Pinecone.
- Ensure HTTPS and secure cookies in production.

## Security & Compliance ğŸ”
- JWT stored in cookie `token`.
- Recommendations: set HttpOnly, Secure, SameSite; rotate `JWT_SECRET`; validate inputs.

## Observability ğŸ‘€
- Basic console logs. Consider adding request logging and error tracking.

## Performance & Cost âš¡
- Embedding dimension 768; topK default 2 for memory queries.
- Consider batching embeddings and caching frequent prompts.

## Roadmap ğŸ—ºï¸
- [ ] Streaming responses over Socket.IO
- [ ] REST endpoints for messages and chat listing
- [ ] Rate limiting and input validation
- [ ] Add tests and linting

## Changelog ğŸ“
- 2025-08-21: Initial complete README based on current code.

## License ğŸ“„
- TBD

## FAQ & Troubleshooting â“ğŸ’¡
- MongoDB connection error: verify `MONGODB_URI` and network access.
- Unauthorized/401: ensure `token` cookie is present; set `JWT_SECRET` and re-login.
- Pinecone errors: ensure `PINECONE_API_KEY` and index `chatgpt` (dim 768) exist.
- No AI response: check `GOOGLE_API_KEY` and model availability.
